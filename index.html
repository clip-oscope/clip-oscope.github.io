
<!DOCTYPE html
    PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<!-- saved from url=(0045)http://6.869.csail.mit.edu/fa19/schedule.html -->
<html xmlns="http://www.w3.org/1999/xhtml">
<link href="https://fonts.cdnfonts.com/css/caveat" rel="stylesheet">
<style>
    @import url('https://fonts.cdnfonts.com/css/caveat');
</style>
<!-- <script src="./assets/teaser-data.js"></script> -->


<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>CLIP Under the Microscope</title>
    <link href="style.css" rel="stylesheet" type="text/css">
    <meta name="description"
        content="Project page for &#39;CLIP Under the Microscope: A Fine-Grained Analysis of Multi-Object Representation.&#39;">
    <!-- <link rel="icon" href="./assets/showlab.ico"> -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"
        integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
    <style>

    </style>

</head>

<body>
    <p class="title">CLIP Under the Microscope: A Fine-Grained Analysis of Multi-Object Representation</p>
    <p class="author">
        <span class="author">
            Reza Abbasi
        </span>
        <span class="author">
            Ali Nazari
        </span>
        <span class="author">
            Aminreza Sefid
        </span>
        <span class="author">
            Mohammadali Banayeeanzade
        </span>
        <span class="author">
            Mohammad Hossein Rohban
        </span>
        <span class="author"></span>
            Mahdieh Soleymani Baghshah
        </span>
    </p>

    <p class="affiliations">
        <span class="author">
            Sharif University of Technology, Tehran, Iran
        </span>
    </p>


	
    
	
    <div style="text-align: center">
        <img src="./assets/1.svg" alt="" width="1500px" style="max-width: 100%; height: auto;;" >
        <p style="text-align: center">
            <em>Evaluation Overview</em>
        </p>
    </div>  
	
    
    
    <p class="section">&nbsp;</p>

    <div class="container">
        <table width="1080px" border="0" align="center">
            <tbody>
                <tr>
                    
                </tr>
                <tr> <br /> </tr>
                <tr align="center"></tr>
            </tbody>
        </table>
        &nbsp;
        <p class="section">&nbsp;</p>

	<p class="move-down">&nbsp;</p>
        <p style="text-align: center;"><span class="section"><b>Abstract</b></span></p>
        <p style="text-align: justify; display: flex; justify-content: center; max-width: 800px; margin:auto"> 
            Contrastive Language-Image Pre-training (CLIP) models excel in zero-shot classification, yet face challenges in complex multi-object scenarios. This study offers a comprehensive analysis of CLIP’s limitations in these contexts using a specialized dataset, ComCO, designed to evaluate CLIP’s encoders in diverse multi-object scenarios. Our findings reveal significant biases: the text encoder prioritizes first-mentioned objects, and the image encoder favors larger objects. Through retrieval and classification tasks, we quantify these biases across multiple CLIP variants and trace their origins to CLIP’s training process, supported by analyses of the LAION dataset and training progression. Our image-text matching experiments show substantial performance drops when object size or token order changes, underscoring CLIP’s instability with rephrased but semantically similar captions. Extending this to longer captions and text-to-image models like Stable Diffusion, we demonstrate how prompt order influences object prominence in generated images.
        <p class="section">&nbsp;</p>

        <p class="section"><b>Introduction</b></p>
        <div style="text-align: center">
            <p>
                The fusion of vision and language in artificial intelligence has led to the rise of powerful 
                <strong>Vision-Language Models (VLMs)</strong>, with OpenAI’s 
                <a href="https://openai.com/research/clip" target="_blank">Contrastive Language-Image Pre-training (CLIP)</a> 
                emerging as a groundbreaking approach. CLIP has demonstrated remarkable success in zero-shot classification and 
                multimodal tasks, but its limitations in multi-object scenarios remain underexplored.
            </p>
    
            <p>
                In <em>CLIP Under the Microscope: A Fine-Grained Analysis of Multi-Object Representation</em>, we systematically 
                investigate CLIP’s performance when processing images with multiple objects. Using our newly developed 
                <strong>ComCO dataset</strong>, we uncover key biases in CLIP’s encoders: the <strong>text encoder</strong> favors 
                the first-mentioned object in captions, while the <strong>image encoder</strong> prioritizes larger objects. These 
                biases result in systematic inconsistencies, particularly when captions are restructured or objects are resized.
            </p>
    
            <p>
                Through rigorous experimentation, we trace the origins of these biases to CLIP’s contrastive training process and 
                dataset characteristics. Our findings extend beyond CLIP, influencing broader applications in 
                <strong>text-to-image generation</strong> and <strong>multimodal large language models (MLLMs)</strong>.
            </p>
    
            <span class="link-block">
                <a href="https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <svg class="svg-inline--fa fa-database fa-w-14" aria-hidden="true" focusable="false" data-prefix="fas" data-icon="database" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" data-fa-i2svg=""><path fill="currentColor" d="M448 73.143v45.714C448 159.143 347.667 192 224 192S0 159.143 0 118.857V73.143C0 32.857 100.333 0 224 0s224 32.857 224 73.143zM448 176v102.857C448 319.143 347.667 352 224 352S0 319.143 0 278.857V176c48.125 33.143 136.208 48.572 224 48.572S399.874 209.143 448 176zm0 160v102.857C448 479.143 347.667 512 224 512S0 479.143 0 438.857V336c48.125 33.143 136.208 48.572 224 48.572S399.874 369.143 448 336z"></path></svg><!-- <i class="fas fa-database"></i> Font Awesome fontawesome.com -->
                  </span>
                  <span>Dataset</span>
                </a>
            </span>
        </div>
	<p class="section">&nbsp;</p>
    <p class="section"><b>Methodology</b></p>
    <p class="subsection"><b>Dataset</b></p>

    

    <div style="display: flex; justify-content: center; align-items: center;">
        <div style="text-align: center; margin-right: 20px;">
            <img src="./assets/3.png" alt="" style="max-width: 100%; height: 500px;" />
            <p><em>Examples of SimCO dataset</em></p>
        </div>
    
        <div style="text-align: center;">
            <img src="./assets/4.png" alt="" style="max-width: 100%; height: 500px;" />
            <p><em>Examples of ComCO dataset</em></p>
        </div>
    </div>
    <p class="subsection"><b>Experimental Setup</b></p>
    <div style="display: flex; justify-content: center; align-items: center;">
        <div style="text-align: center; margin-right: 20px;">
            <img src="./assets/clip-ior.png" alt="" style="max-width: 100%;" />
            <p><em>Bias on bigger object</em></p>
        </div>
    
        <div style="text-align: center;">
            <img src="./assets/clip-tor.png" alt="" style="max-width: 100%;" />
            <p><em>Bias on first object</em></p>
        </div>
    </div>

</br>

    <div style="display: flex; justify-content: center; align-items: center;">
        <div style="text-align: center; margin-right: 20px;">
            <img src="./assets/clip-ioc.png" alt="" style="max-width: 100%;" />
            <p><em>Bias on bigger object</em></p>
        </div>
    
        <div style="text-align: center;position: relative; bottom: 10px;">
            <img src="./assets/clip-toc.png" alt="" style="max-width: 100%;" />
            <p style="position: relative; top: 24px;"><em>Bias on first object</em></p>
        </div>
    </div>


            
        
    


    <p class="section">&nbsp;</p>

    <p class="section"><b>Codes</b></p>
    <p>Note: The code referenced here is still under development and might not be fully optimized or clean yet. Please visit the following link to access the code:</p>
    <a href="https://github.com/clip-analysis/Clip-Analysis">GitHub Repository for Clip Analysis</a>
    
    </br>
    

        <p class="section" id="bibtex"><b>Bibtex</b></p>
        <table border="0">
            <tbody>
                <pre style=" display: block;
                    background: #eee;
                    white-space: pre;
                    -webkit-overflow-scrolling: touch;
                    max-width: 100%;
                    min-width: 100px;
                    border-radius: 20px;
                    ">

    @article{{}2024Clip Analysis,
        title={Analyzing CLIP’s Performance Limitations in Multi-Object Scenarios: A Controlled High-Resolution Study},
        author={},
        journal={arXiv preprint arXiv: {num}},
        year={2024}
    }       

			    </pre>
            </tbody>
        </table>
        
    </div>

    <footer class="footer">
        <p style="text-align: center;">
            This page was adapted from <a href="https://tuneavideo.github.io/">Tune-A-Video</a>.
        </p>
    </footer>


</body>

<script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-WLX2Z5QLG8');
 </script>
 <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
 <script type="text/javascript">
    $(document).ready(function () {

        if (localStorage.getItem("my_app_name_here-quote-scroll") != null) {
            $(window).scrollTop(localStorage.getItem("my_app_name_here-quote-scroll"));
        }

        $(window).on("scroll", function() {
            localStorage.setItem("my_app_name_here-quote-scroll", $(window).scrollTop());
        });

      });
 </script>

 <script>
    function prompt_on(prompt_element) {
        prompt_element.classList.add("caption-active");
    }

    function prompt_off(prompt_element) {
        prompt_element.classList.remove("caption-active");
    }

    function toggle_prompt(active_prompt_id, inactive_prompt_ids, result_id) {
        let active_prompt = document.getElementById(active_prompt_id);
        prompt_on(active_prompt);
        for (let i = 0; i < inactive_prompt_ids.length; i++) {
            let inactive_prompt = document.getElementById(inactive_prompt_ids[i]);
            prompt_off(inactive_prompt);
        }

        let result = document.getElementById(result_id);
        result.src = file_paths[active_prompt_id];
    }

    function toggle_prompts(active_prompt_id1, active_prompt_id2, inactive_prompt_ids1, inactive_prompt_ids2, result_id1, result_id2 ) {
    toggle_prompt(active_prompt_id1, inactive_prompt_ids1, result_id1);
    toggle_prompt(active_prompt_id2, inactive_prompt_ids2, result_id2);
    }


 </script>

 <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
 <script src="https://cdn.jsdelivr.net/npm/popper.js@1.12.9/dist/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
 <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>

</html>